{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCSmmt8MOkDu"
   },
   "source": [
    "# EYE FOR BLIND\n",
    "This notebook will be used to prepare the capstone project 'Eye for Blind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:42.724498Z",
     "iopub.status.busy": "2021-12-20T07:29:42.724216Z",
     "iopub.status.idle": "2021-12-20T07:29:42.734967Z",
     "shell.execute_reply": "2021-12-20T07:29:42.734275Z",
     "shell.execute_reply.started": "2021-12-20T07:29:42.724466Z"
    },
    "id": "YrKk6r7xOkD1"
   },
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np # for scientific computing\n",
    "import pandas as pd # for data manipulation, processing and analysis\n",
    "import matplotlib.pyplot as plt # plotting \n",
    "import seaborn as sns # visualization\n",
    "import glob # to return all file paths which matches specific pattern\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split #to split the data\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 # pre-trained model\n",
    "from tensorflow.keras.models import Model  # neural network model\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import load_img # to load an image from files as PIl image object\n",
    "from keras.preprocessing.text import Tokenizer # vectorizing a text\n",
    "\n",
    "from skimage import io # to read/ write images\n",
    "\n",
    "import collections # containers used to store data\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    " \n",
    "from tqdm import tqdm # for creating Progress\n",
    "from PIL import Image # Python Image Library\n",
    "import IPython\n",
    "from IPython import display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrMoJwZvOkD4"
   },
   "source": [
    "Let's read the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvr28dLOOkD6"
   },
   "source": [
    "## Data understanding\n",
    "1.Import the dataset and read image & captions into two seperate variables\n",
    "\n",
    "2.Visualise both the images & text present in the dataset\n",
    "\n",
    "3.Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "4.Create a list which contains all the captions & path\n",
    "\n",
    "5.Visualise the top 30 occuring words in the captions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:45.553551Z",
     "iopub.status.busy": "2021-12-20T07:29:45.552795Z",
     "iopub.status.idle": "2021-12-20T07:29:45.591743Z",
     "shell.execute_reply": "2021-12-20T07:29:45.590988Z",
     "shell.execute_reply.started": "2021-12-20T07:29:45.553504Z"
    },
    "id": "ejhMQbY0OkD7",
    "outputId": "90f2a8d9-9ccf-4140-86e4-3f7dd3f47b11"
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the image into a seperate variable\n",
    "\n",
    "images= '../input/flickr8k/Images'\n",
    "\n",
    "all_imgs = glob.glob(images + '/*.jpg',recursive=True)\n",
    "print(\"The total images present in the dataset: {}\".format(len(all_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:46.524891Z",
     "iopub.status.busy": "2021-12-20T07:29:46.524240Z",
     "iopub.status.idle": "2021-12-20T07:29:47.110324Z",
     "shell.execute_reply": "2021-12-20T07:29:47.109705Z",
     "shell.execute_reply.started": "2021-12-20T07:29:46.524850Z"
    },
    "id": "rVjlc_CJOkD8",
    "outputId": "26fd1d04-7c00-49aa-c194-c0beabda94c3"
   },
   "outputs": [],
   "source": [
    "#Visualise images\n",
    "Img = all_imgs[0:3]\n",
    "fig, axes = plt.subplots(1,3)\n",
    "fig.set_figwidth(20)\n",
    "\n",
    "for ax,image in zip(axes,Img):\n",
    "    ax.imshow(io.imread(image),cmap=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:47.122552Z",
     "iopub.status.busy": "2021-12-20T07:29:47.121846Z",
     "iopub.status.idle": "2021-12-20T07:29:47.131251Z",
     "shell.execute_reply": "2021-12-20T07:29:47.130345Z",
     "shell.execute_reply.started": "2021-12-20T07:29:47.122509Z"
    },
    "id": "UHMQZpckOkD9",
    "outputId": "083871f2-c0c1-4962-a26e-5125b795c6a7"
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the text file into a seperate variable\n",
    "\n",
    "text_file = '../input/flickr8k/captions.txt'\n",
    "\n",
    "def load_doc(text_file):\n",
    "    open_file = open(text_file, 'r')\n",
    "    text = open_file.read()\n",
    "    open_file.close()\n",
    "    return text\n",
    "\n",
    "doc = load_doc(text_file)\n",
    "print(doc[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNLdvz7NOkD_"
   },
   "source": [
    "Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:48.322326Z",
     "iopub.status.busy": "2021-12-20T07:29:48.322053Z",
     "iopub.status.idle": "2021-12-20T07:29:48.406468Z",
     "shell.execute_reply": "2021-12-20T07:29:48.405607Z",
     "shell.execute_reply.started": "2021-12-20T07:29:48.322273Z"
    },
    "id": "uFGwPennOkEB",
    "outputId": "0f7c2e72-e4d1-417a-c2d4-64ca97d3808e"
   },
   "outputs": [],
   "source": [
    "image_path='../input/flickr8k/Images/'\n",
    "\n",
    "all_img_id=[] #store all the image id here\n",
    "all_img_vector=[] #store all the image path here\n",
    "annotations=[] #store all the captions here\n",
    "\n",
    "with open('../input/flickr8k/captions.txt','r') as fc:\n",
    "    next(fc)\n",
    "    for line in fc:\n",
    "        split_line = line.split(',')\n",
    "        all_img_id.append(split_line[0])\n",
    "        all_img_vector.append(image_path + split_line[0])\n",
    "        annotations.append(split_line[1].rstrip('\\n.'))\n",
    "\n",
    "df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:48.851417Z",
     "iopub.status.busy": "2021-12-20T07:29:48.850666Z",
     "iopub.status.idle": "2021-12-20T07:29:48.872037Z",
     "shell.execute_reply": "2021-12-20T07:29:48.871307Z",
     "shell.execute_reply.started": "2021-12-20T07:29:48.851323Z"
    },
    "id": "Ym-a602bOkEC",
    "outputId": "592a1f10-3332-4a00-e12b-fffc24efabfc"
   },
   "outputs": [],
   "source": [
    "#Create a list which contains all the captions\n",
    "annotations=['<start>' + ' ' +  line + ' ' + '<end>' for line in annotations]\n",
    "\n",
    "#add the <start> & <end> token to all those captions as well\n",
    "\n",
    "#Create a list which contains all the path to the images\n",
    "all_img_path=all_img_vector#write your code here\n",
    "\n",
    "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
    "print(\"Total images present in the dataset: \" + str(len(all_img_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:49.379173Z",
     "iopub.status.busy": "2021-12-20T07:29:49.378841Z",
     "iopub.status.idle": "2021-12-20T07:29:49.389280Z",
     "shell.execute_reply": "2021-12-20T07:29:49.388649Z",
     "shell.execute_reply.started": "2021-12-20T07:29:49.379132Z"
    },
    "id": "Eyya0R7q4wA_",
    "outputId": "732a512f-6d91-4294-b024-7e7c7a88cd47"
   },
   "outputs": [],
   "source": [
    "annotations[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:49.941019Z",
     "iopub.status.busy": "2021-12-20T07:29:49.940508Z",
     "iopub.status.idle": "2021-12-20T07:29:50.151769Z",
     "shell.execute_reply": "2021-12-20T07:29:50.151005Z",
     "shell.execute_reply.started": "2021-12-20T07:29:49.940979Z"
    },
    "id": "MoXDjLZ9OkED",
    "outputId": "f8b77c62-f7aa-4714-ab46-c6ddeb5df278"
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary & the counter for the captions\n",
    "\n",
    "vocabulary= [word.lower() for line in annotations for word in line.split()]\n",
    "\n",
    "val_count=collections.Counter(vocabulary)\n",
    "val_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:50.537876Z",
     "iopub.status.busy": "2021-12-20T07:29:50.537093Z",
     "iopub.status.idle": "2021-12-20T07:29:51.142740Z",
     "shell.execute_reply": "2021-12-20T07:29:51.141966Z",
     "shell.execute_reply.started": "2021-12-20T07:29:50.537829Z"
    },
    "id": "XY3Fi02QOkED",
    "outputId": "717c535f-85cc-4cbf-f0b3-83aeaf292bb2"
   },
   "outputs": [],
   "source": [
    "#Visualise the top 30 occuring words in the captions\n",
    "\n",
    "\n",
    "#write your code here\n",
    "top30 = val_count.most_common(30)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for word_count in top30 : \n",
    "    words.append(word_count[0])\n",
    "    counts.append(word_count[1])\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Top 30 words in the vocabulary')\n",
    "plt.xlabel('word')\n",
    "plt.ylabel('count')\n",
    "plot = sns.barplot(x=words, y=counts)\n",
    "for p in plot.patches:\n",
    "    plot.annotate(format(int(p.get_height())), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), \n",
    "                   textcoords = 'offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:51.144978Z",
     "iopub.status.busy": "2021-12-20T07:29:51.144255Z",
     "iopub.status.idle": "2021-12-20T07:29:51.363410Z",
     "shell.execute_reply": "2021-12-20T07:29:51.362676Z",
     "shell.execute_reply.started": "2021-12-20T07:29:51.144935Z"
    },
    "id": "-w13WSo75frd",
    "outputId": "6436c74d-060d-4a7f-d276-d63378882b08"
   },
   "outputs": [],
   "source": [
    "# visualizing the random image and caption:\n",
    "\n",
    "import random as rn \n",
    "    # image\n",
    "random_index = rn.randint(0,len(all_imgs))\n",
    "image_id = df.loc[random_index,'ID']\n",
    "image = plt.imread(images+'/'+ image_id)\n",
    "plt.title('Image ID : ' +image_id )\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print('Image Shape : ', image.shape,'\\n')\n",
    "    \n",
    "    # captions\n",
    "condition = df['ID'] == image_id\n",
    "print('Captions for Image ID # ', image_id , ' : ')\n",
    "print(df.loc[condition,'Captions'].values, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnOBq4JjOkEE"
   },
   "source": [
    "## Pre-Processing the captions\n",
    "1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \n",
    "This gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n",
    "\n",
    "2.Replace all other words with the unknown token \"UNK\" .\n",
    "\n",
    "3.Create word-to-index and index-to-word mappings.\n",
    "\n",
    "4.Pad all sequences to be the same length as the longest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:52.754575Z",
     "iopub.status.busy": "2021-12-20T07:29:52.754322Z",
     "iopub.status.idle": "2021-12-20T07:29:52.759233Z",
     "shell.execute_reply": "2021-12-20T07:29:52.758550Z",
     "shell.execute_reply.started": "2021-12-20T07:29:52.754546Z"
    },
    "id": "JDgcyYhyOkEE"
   },
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "\n",
    "#your code here\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "num_words = 5000, \n",
    "filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ',\n",
    "lower = True, \n",
    "split = \" \", \n",
    "char_level = False, \n",
    "oov_token = '<unk>',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:53.848447Z",
     "iopub.status.busy": "2021-12-20T07:29:53.846627Z",
     "iopub.status.idle": "2021-12-20T07:29:55.365883Z",
     "shell.execute_reply": "2021-12-20T07:29:55.365143Z",
     "shell.execute_reply.started": "2021-12-20T07:29:53.848402Z"
    },
    "id": "6P5_sYSWOkEF",
    "outputId": "9560ed53-1a4f-48bf-c47e-788e58862bae"
   },
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings.\n",
    "\n",
    "#your code here\n",
    "\n",
    "tokenizer.fit_on_texts(annotations)\n",
    "\n",
    "# Converting sentences to sequences of word token indexes\n",
    "caption_text_sequences = tokenizer.texts_to_sequences(annotations)\n",
    "caption_text_sequences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:55.368158Z",
     "iopub.status.busy": "2021-12-20T07:29:55.367874Z",
     "iopub.status.idle": "2021-12-20T07:29:55.372262Z",
     "shell.execute_reply": "2021-12-20T07:29:55.371497Z",
     "shell.execute_reply.started": "2021-12-20T07:29:55.368120Z"
    },
    "id": "J-KQ3lfc57UM"
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index['PAD'] = 0\n",
    "tokenizer.index_word[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:29:55.374455Z",
     "iopub.status.busy": "2021-12-20T07:29:55.373549Z",
     "iopub.status.idle": "2021-12-20T07:29:56.005196Z",
     "shell.execute_reply": "2021-12-20T07:29:56.004349Z",
     "shell.execute_reply.started": "2021-12-20T07:29:55.374418Z"
    },
    "id": "PztsZVNvOkEF",
    "outputId": "d52c6085-b1ff-4369-ccf1-f9533ff7ac9b"
   },
   "outputs": [],
   "source": [
    "# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n",
    "\n",
    "#your code here\n",
    "words_count = tokenizer.get_config()['word_counts']\n",
    "\n",
    "words_count_df = pd.DataFrame.from_dict(data = json.loads(words_count), orient='index', columns=['count'])\n",
    "top_30 = words_count_df.sort_values(by='count', ascending=False)[:30]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Top 30 words in the vocabulary')\n",
    "plt.xlabel('word')\n",
    "plt.ylabel('count')\n",
    "plot = sns.barplot(x=top_30.index , y=top_30['count'])\n",
    "for p in plot.patches:\n",
    "    plot.annotate(format(int(p.get_height())), \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   xytext = (0, 9), \n",
    "                   textcoords = 'offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:30:50.106904Z",
     "iopub.status.busy": "2021-12-20T07:30:50.106435Z",
     "iopub.status.idle": "2021-12-20T07:30:50.413717Z",
     "shell.execute_reply": "2021-12-20T07:30:50.412271Z",
     "shell.execute_reply.started": "2021-12-20T07:30:50.106868Z"
    },
    "id": "COlI57B_OkEG",
    "outputId": "6e12b134-0c6b-40f6-aee7-39351871f15a"
   },
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions, store it to a vairable\n",
    "\n",
    "caption_sequences_len=[len(seq) for seq in caption_text_sequences] #storing all lengths in list.Can be used if needed in future\n",
    "longest_word_length= max(caption_sequences_len) #Python list method max returns the elements from the list with maximum value.\n",
    "\n",
    "\n",
    "cap_vector=  tf.keras.preprocessing.sequence.pad_sequences(caption_sequences, padding='post',maxlen=longest_word_length,\n",
    "                                                          dtype='int32', value=0)\n",
    "\n",
    "\n",
    "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ahky8kZ5OkEG"
   },
   "source": [
    "## Pre-processing the images\n",
    "\n",
    "1.Resize them into the shape of (299, 299)\n",
    "\n",
    "3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9I5iL0LOkEG"
   },
   "source": [
    "### FAQs on how to resize the images::\n",
    "* Since you have a list which contains all the image path, you need to first convert them to a dataset using <i>tf.data.Dataset.from_tensor_slices</i>. Once you have created a dataset consisting of image paths, you need to apply a function to the dataset which will apply the necessary preprocessing to each image. \n",
    "* This function should resize them and also should do the necessary preprocessing that it is in correct format for InceptionV3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:30:56.951970Z",
     "iopub.status.busy": "2021-12-20T07:30:56.951703Z",
     "iopub.status.idle": "2021-12-20T07:30:56.957062Z",
     "shell.execute_reply": "2021-12-20T07:30:56.954682Z",
     "shell.execute_reply.started": "2021-12-20T07:30:56.951938Z"
    },
    "id": "pWecRyPoTYTw"
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE= (299, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:30:58.119454Z",
     "iopub.status.busy": "2021-12-20T07:30:58.118845Z",
     "iopub.status.idle": "2021-12-20T07:30:58.156735Z",
     "shell.execute_reply": "2021-12-20T07:30:58.156097Z",
     "shell.execute_reply.started": "2021-12-20T07:30:58.119401Z"
    },
    "id": "AJ2ZVa61OkEH"
   },
   "outputs": [],
   "source": [
    "#write your code here to create the dataset consisting of image paths\n",
    "all_img_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:30:59.767056Z",
     "iopub.status.busy": "2021-12-20T07:30:59.766386Z",
     "iopub.status.idle": "2021-12-20T07:30:59.772095Z",
     "shell.execute_reply": "2021-12-20T07:30:59.771154Z",
     "shell.execute_reply.started": "2021-12-20T07:30:59.767020Z"
    },
    "id": "5git2ddPOkEH"
   },
   "outputs": [],
   "source": [
    "#write your code here for creating the function. This function should return images & their path\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:31:34.321403Z",
     "iopub.status.busy": "2021-12-20T07:31:34.321060Z",
     "iopub.status.idle": "2021-12-20T07:31:34.544466Z",
     "shell.execute_reply": "2021-12-20T07:31:34.543774Z",
     "shell.execute_reply.started": "2021-12-20T07:31:34.321361Z"
    },
    "id": "GFu4wWJoOkEH",
    "outputId": "4632542f-2d61-444f-cde1-269b6de5cc99"
   },
   "outputs": [],
   "source": [
    "#write your code here for applying the function to the image path dataset, such that the transformed dataset should contain images & their path\n",
    "\n",
    "\n",
    "image,image_path = load_image(r\"../input/flickr8k/Images/41999070_838089137e.jpg\")\n",
    "print(\"Shape after resize :\", image.shape)\n",
    "plt.imshow(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:31:59.279931Z",
     "iopub.status.busy": "2021-12-20T07:31:59.279670Z",
     "iopub.status.idle": "2021-12-20T07:31:59.328189Z",
     "shell.execute_reply": "2021-12-20T07:31:59.327499Z",
     "shell.execute_reply.started": "2021-12-20T07:31:59.279901Z"
    },
    "id": "vkksTZoQA8kf"
   },
   "outputs": [],
   "source": [
    "encode_train_set = sorted(set(all_img_vector))\n",
    "\n",
    "feature_dict = {}\n",
    "\n",
    "image_data_set = tf.data.Dataset.from_tensor_slices(encode_train_set)\n",
    "image_data_set = image_data_set.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:32:01.930801Z",
     "iopub.status.busy": "2021-12-20T07:32:01.930533Z",
     "iopub.status.idle": "2021-12-20T07:32:01.941045Z",
     "shell.execute_reply": "2021-12-20T07:32:01.940101Z",
     "shell.execute_reply.started": "2021-12-20T07:32:01.930769Z"
    },
    "id": "rTWYGQSsBCyB",
    "outputId": "bff2ad22-ee45-4b69-a688-46643b0174da"
   },
   "outputs": [],
   "source": [
    "image_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jYikjuKOkEI"
   },
   "source": [
    "## Load the pretrained Imagenet weights of Inception net V3\n",
    "\n",
    "1.To save the memory(RAM) from getting exhausted, extract the features of the images using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n",
    "\n",
    "2.The shape of the output of this layer is 8x8x2048. \n",
    "\n",
    "3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:32:32.901299Z",
     "iopub.status.busy": "2021-12-20T07:32:32.901036Z",
     "iopub.status.idle": "2021-12-20T07:32:35.565604Z",
     "shell.execute_reply": "2021-12-20T07:32:35.564883Z",
     "shell.execute_reply.started": "2021-12-20T07:32:32.901257Z"
    },
    "id": "huSMBYW6OkEI"
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "new_input = image_model.input #write code here to get the input of the image_model\n",
    "hidden_layer = image_model.layers[-1].output #write code here to get the output of the image_model\n",
    "\n",
    "image_features_extract_model = tf.compat.v1.keras.Model(new_input, hidden_layer)  #build the final model using both input & output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:32:36.140195Z",
     "iopub.status.busy": "2021-12-20T07:32:36.139566Z",
     "iopub.status.idle": "2021-12-20T07:32:36.167408Z",
     "shell.execute_reply": "2021-12-20T07:32:36.166353Z",
     "shell.execute_reply.started": "2021-12-20T07:32:36.140152Z"
    },
    "id": "iMCGAOd_W-_y",
    "outputId": "8eb98270-02e3-4570-da56-ebff6cb2e24c"
   },
   "outputs": [],
   "source": [
    "all_img_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:36:24.857986Z",
     "iopub.status.busy": "2021-12-20T07:36:24.857083Z",
     "iopub.status.idle": "2021-12-20T07:36:24.893217Z",
     "shell.execute_reply": "2021-12-20T07:36:24.892555Z",
     "shell.execute_reply.started": "2021-12-20T07:36:24.857932Z"
    },
    "id": "PsteGKOcXEt3"
   },
   "outputs": [],
   "source": [
    "Img_Data_List = sorted(set(all_img_vector)) \n",
    "\n",
    "# Creating a Dataset using tf.data.Dataset.from_tensor_slice\n",
    "Image_Dataset_New = tf.data.Dataset.from_tensor_slices(Img_Data_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:36:36.317259Z",
     "iopub.status.busy": "2021-12-20T07:36:36.316801Z",
     "iopub.status.idle": "2021-12-20T07:36:36.341047Z",
     "shell.execute_reply": "2021-12-20T07:36:36.340358Z",
     "shell.execute_reply.started": "2021-12-20T07:36:36.317207Z"
    },
    "id": "j00a5SZ-XJvc"
   },
   "outputs": [],
   "source": [
    "Image_Dataset_New = Image_Dataset_New.map(load_image,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:36:51.959080Z",
     "iopub.status.busy": "2021-12-20T07:36:51.958525Z",
     "iopub.status.idle": "2021-12-20T07:36:51.964612Z",
     "shell.execute_reply": "2021-12-20T07:36:51.963659Z",
     "shell.execute_reply.started": "2021-12-20T07:36:51.959043Z"
    },
    "id": "3vbhWTU6XLV0"
   },
   "outputs": [],
   "source": [
    "Image_Dataset_New= Image_Dataset_New.batch(64,drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:37:20.306692Z",
     "iopub.status.busy": "2021-12-20T07:37:20.306384Z",
     "iopub.status.idle": "2021-12-20T07:38:42.298798Z",
     "shell.execute_reply": "2021-12-20T07:38:42.297981Z",
     "shell.execute_reply.started": "2021-12-20T07:37:20.306657Z"
    },
    "id": "nEgkE6eSOkEI",
    "outputId": "1336d927-1217-431d-87b5-3108525ec97e"
   },
   "outputs": [],
   "source": [
    "# write the code to apply the feature_extraction model to your earlier created dataset which contained images & their respective paths\n",
    "# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\n",
    "\n",
    "image_features_dict={}\n",
    "for image, image_path in tqdm(Image_Dataset_New): #using tqdm as progress bar\n",
    "    features_for_batch = image_features_extract_model(image) #feeding images from above created dataset to Inception v3 which we build above\n",
    "    features_for_batch_flattened = tf.reshape(features_for_batch,\n",
    "                             (features_for_batch.shape[0], -1, features_for_batch.shape[3])) ##We are sqeezing/squashing \n",
    "                                   \n",
    "    for batch_feat, path in zip(features_for_batch_flattened, image_path):\n",
    "        feature_path = path.numpy().decode(\"utf-8\")\n",
    "        image_features_dict[feature_path] =  batch_feat.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEdCupTiOkEJ"
   },
   "source": [
    "### FAQs on how to store the features:\n",
    "* You can store the features using a dictionary with the path as the key and values as the feature extracted by the inception net v3 model OR\n",
    "* You can store using numpy(np.save) to store the resulting vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysTc-GEiOkEJ"
   },
   "source": [
    "## Dataset creation\n",
    "1.Apply train_test_split on both image path & captions to create the train & test list. Create the train-test spliit using 80-20 ratio & random state = 42\n",
    "\n",
    "2.Create a function which maps the image path to their feature. \n",
    "\n",
    "3.Create a builder function to create train & test dataset & apply the function created earlier to transform the dataset\n",
    "\n",
    "2.Make sure you have done Shuffle and batch while building the dataset\n",
    "\n",
    "3.The shape of each image in the dataset after building should be (batch_size, 8*8, 2048)\n",
    "\n",
    "4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:38:42.301228Z",
     "iopub.status.busy": "2021-12-20T07:38:42.300949Z",
     "iopub.status.idle": "2021-12-20T07:38:42.330785Z",
     "shell.execute_reply": "2021-12-20T07:38:42.330082Z",
     "shell.execute_reply.started": "2021-12-20T07:38:42.301188Z"
    },
    "id": "GfqDN5GBOkEJ"
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "\n",
    "path_train, path_test, cap_train, cap_test = train_test_split(all_img_vector,cap_vector,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:39:51.072818Z",
     "iopub.status.busy": "2021-12-20T07:39:51.072131Z",
     "iopub.status.idle": "2021-12-20T07:39:51.079966Z",
     "shell.execute_reply": "2021-12-20T07:39:51.079256Z",
     "shell.execute_reply.started": "2021-12-20T07:39:51.072777Z"
    },
    "id": "7cZTyYDpOkEJ",
    "outputId": "03e95a2e-97f7-448d-e482-2553644d7448"
   },
   "outputs": [],
   "source": [
    "print(\"Training data for images: \" + str(len(path_train)))\n",
    "print(\"Testing data for images: \" + str(len(path_test)))\n",
    "print(\"Training data for Captions: \" + str(len(cap_train)))\n",
    "print(\"Testing data for Captions: \" + str(len(cap_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:39:52.769860Z",
     "iopub.status.busy": "2021-12-20T07:39:52.769035Z",
     "iopub.status.idle": "2021-12-20T07:39:52.774455Z",
     "shell.execute_reply": "2021-12-20T07:39:52.773327Z",
     "shell.execute_reply.started": "2021-12-20T07:39:52.769812Z"
    },
    "id": "mYcA0hF-OkEJ"
   },
   "outputs": [],
   "source": [
    "# Create a function which maps the image path to their feature. \n",
    "# This function will take the image_path & caption and return it's feature & respective caption.\n",
    "\n",
    "def map_func(image,captions):\n",
    "        image_final = image_features_dict[image.decode('utf-8')]\n",
    "        return image_final,captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2beBiYeOkEK"
   },
   "source": [
    "### FAQs on how to load the features:\n",
    "* You can load the features using the dictionary created earlier OR\n",
    "* You can store using numpy(np.load) to load the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:40:35.999369Z",
     "iopub.status.busy": "2021-12-20T07:40:35.999020Z",
     "iopub.status.idle": "2021-12-20T07:40:36.012360Z",
     "shell.execute_reply": "2021-12-20T07:40:36.011338Z",
     "shell.execute_reply.started": "2021-12-20T07:40:35.999330Z"
    },
    "id": "0KdUo8e_OkEK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a builder function to create dataset which takes in the image path & captions as input\n",
    "# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 256\n",
    "def gen_dataset(images_data, captions_data):\n",
    "    \n",
    "    # Creating a Dataset using tf.data.Dataset.from_tensor_slice \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_data, captions_data))\n",
    "\n",
    "    # num_parallel_calls= tf.data.AUTOTUNE is used, then the number of parallel calls is set dynamically based on available CPU.\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dataset = (\n",
    "     dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True) \n",
    "    .batch(BATCH_SIZE, drop_remainder=False)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    ) \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:40:41.021799Z",
     "iopub.status.busy": "2021-12-20T07:40:41.021208Z",
     "iopub.status.idle": "2021-12-20T07:40:41.199299Z",
     "shell.execute_reply": "2021-12-20T07:40:41.198597Z",
     "shell.execute_reply.started": "2021-12-20T07:40:41.021761Z"
    },
    "id": "VSbCONLMOkEK",
    "outputId": "1b96fe9a-51dd-4778-c8a2-dce33c7551fc"
   },
   "outputs": [],
   "source": [
    "train_dataset=gen_dataset(path_train,cap_train)\n",
    "test_dataset=gen_dataset(path_test,cap_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:40:43.229760Z",
     "iopub.status.busy": "2021-12-20T07:40:43.229206Z",
     "iopub.status.idle": "2021-12-20T07:40:43.804300Z",
     "shell.execute_reply": "2021-12-20T07:40:43.803510Z",
     "shell.execute_reply.started": "2021-12-20T07:40:43.229723Z"
    },
    "id": "QIpRidqROkEK"
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n",
    "print(sample_cap_batch.shape) #(batch_size,max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOoW2MFEOkEK"
   },
   "source": [
    "## Model Building\n",
    "1.Set the parameters\n",
    "\n",
    "2.Build the Encoder, Attention model & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:40:45.883640Z",
     "iopub.status.busy": "2021-12-20T07:40:45.882706Z",
     "iopub.status.idle": "2021-12-20T07:40:45.889242Z",
     "shell.execute_reply": "2021-12-20T07:40:45.888393Z",
     "shell.execute_reply.started": "2021-12-20T07:40:45.883581Z"
    },
    "id": "gkECW7xlOkEL"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256 \n",
    "units = 512\n",
    "vocab_size = 5001\n",
    "train_num_steps =len(path_train) // BATCH_SIZE\n",
    "test_num_steps = len(path_test) // BATCH_SIZE\n",
    "max_length=35\n",
    "features_shape = batch_feat.shape[1]\n",
    "attention_features_shape = batch_feat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2gWjbWAOkEL"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:40:48.104148Z",
     "iopub.status.busy": "2021-12-20T07:40:48.103882Z",
     "iopub.status.idle": "2021-12-20T07:40:48.110568Z",
     "shell.execute_reply": "2021-12-20T07:40:48.109628Z",
     "shell.execute_reply.started": "2021-12-20T07:40:48.104118Z"
    },
    "id": "dds_D8aDOkEL"
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense =tf.keras.layers.Dense(embed_dim) #build your Dense layer with relu activation\n",
    "        \n",
    "    def call(self, features):\n",
    "        features = self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "        features = tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0) #applying relu activation \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:40:51.734218Z",
     "iopub.status.busy": "2021-12-20T07:40:51.733938Z",
     "iopub.status.idle": "2021-12-20T07:40:51.743174Z",
     "shell.execute_reply": "2021-12-20T07:40:51.742304Z",
     "shell.execute_reply.started": "2021-12-20T07:40:51.734189Z"
    },
    "id": "cdwe8ecYOkEL"
   },
   "outputs": [],
   "source": [
    "encoder=Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtwY74HQOkEM"
   },
   "source": [
    "### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:40:59.117224Z",
     "iopub.status.busy": "2021-12-20T07:40:59.116435Z",
     "iopub.status.idle": "2021-12-20T07:40:59.125683Z",
     "shell.execute_reply": "2021-12-20T07:40:59.124877Z",
     "shell.execute_reply.started": "2021-12-20T07:40:59.117172Z"
    },
    "id": "U2attlG6OkEM"
   },
   "outputs": [],
   "source": [
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.W1 =tf.keras.layers.Dense(units) #build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units)#build your Dense layer\n",
    "        self.V = tf.keras.layers.Dense(1)#build your final Dense layer with unit 1\n",
    "        self.units=units\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        #features shape: (batch_size, 8*8, embedding_dim)\n",
    "        # hidden shape: (batch_size, hidden_size)\n",
    "        hidden_with_time_axis =  hidden[:, tf.newaxis]# Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n",
    "        score =tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis)) # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        attention_weights = tf.keras.activations.softmax(self.V(score), axis=1) # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        context_vector = attention_weights * features #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector =tf.reduce_sum(context_vector, axis=1) # reduce the shape to (batch_size, embedding_dim)\n",
    "        \n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1v2A0u0OkEM"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:01.296839Z",
     "iopub.status.busy": "2021-12-20T07:41:01.296019Z",
     "iopub.status.idle": "2021-12-20T07:41:01.302228Z",
     "shell.execute_reply": "2021-12-20T07:41:01.301530Z",
     "shell.execute_reply.started": "2021-12-20T07:41:01.296797Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, Conv2D\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:02.350468Z",
     "iopub.status.busy": "2021-12-20T07:41:02.349610Z",
     "iopub.status.idle": "2021-12-20T07:41:02.360874Z",
     "shell.execute_reply": "2021-12-20T07:41:02.360072Z",
     "shell.execute_reply.started": "2021-12-20T07:41:02.350419Z"
    },
    "id": "xQl9HRlBOkEM"
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units=units\n",
    "        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim,mask_zero=False)#build your Embedding layer\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        self.d1 =tf.keras.layers.Dense(self.units) #build your Dense layer\n",
    "        self.d2 =tf.keras.layers.Dense(vocab_size) #build your Dense layer\n",
    "        #self.dropout = Dropout(0.5)\n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        context_vector, attention_weights =self.attention(features, hidden) #create your context vector & attention weights from attention model\n",
    "        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        output,state =  self.gru(embed)# Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
    "        output = self.d1(output)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
    "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
    "        \n",
    "        return output,state, attention_weights\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:03.357599Z",
     "iopub.status.busy": "2021-12-20T07:41:03.356679Z",
     "iopub.status.idle": "2021-12-20T07:41:03.377009Z",
     "shell.execute_reply": "2021-12-20T07:41:03.376336Z",
     "shell.execute_reply.started": "2021-12-20T07:41:03.357548Z"
    },
    "id": "05DNtB_7OkEM"
   },
   "outputs": [],
   "source": [
    "decoder=Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:04.503961Z",
     "iopub.status.busy": "2021-12-20T07:41:04.503402Z",
     "iopub.status.idle": "2021-12-20T07:41:04.807218Z",
     "shell.execute_reply": "2021-12-20T07:41:04.806484Z",
     "shell.execute_reply.started": "2021-12-20T07:41:04.503924Z"
    },
    "id": "qoVwHMAvOkEN"
   },
   "outputs": [],
   "source": [
    "features=encoder(sample_img_batch)\n",
    "\n",
    "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
    "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
    "\n",
    "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
    "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
    "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
    "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7K_EiEzOkEN"
   },
   "source": [
    "## Model training & optimization\n",
    "1.Set the optimizer & loss object\n",
    "\n",
    "2.Create your checkpoint path\n",
    "\n",
    "3.Create your training & testing step functions\n",
    "\n",
    "4.Create your loss function for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:17.132873Z",
     "iopub.status.busy": "2021-12-20T07:41:17.132126Z",
     "iopub.status.idle": "2021-12-20T07:41:17.139507Z",
     "shell.execute_reply": "2021-12-20T07:41:17.138801Z",
     "shell.execute_reply.started": "2021-12-20T07:41:17.132833Z"
    },
    "id": "eGt4-pcwOkEN"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)#define the optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=tf.keras.losses.Reduction.NONE)#define your loss object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:18.137920Z",
     "iopub.status.busy": "2021-12-20T07:41:18.137655Z",
     "iopub.status.idle": "2021-12-20T07:41:18.143188Z",
     "shell.execute_reply": "2021-12-20T07:41:18.142489Z",
     "shell.execute_reply.started": "2021-12-20T07:41:18.137890Z"
    },
    "id": "4wP5qT16OkEN"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:19.269391Z",
     "iopub.status.busy": "2021-12-20T07:41:19.268635Z",
     "iopub.status.idle": "2021-12-20T07:41:19.276223Z",
     "shell.execute_reply": "2021-12-20T07:41:19.275202Z",
     "shell.execute_reply.started": "2021-12-20T07:41:19.269343Z"
    },
    "id": "EhFFYuHiOkEN"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:20.544426Z",
     "iopub.status.busy": "2021-12-20T07:41:20.543903Z",
     "iopub.status.idle": "2021-12-20T07:41:20.548859Z",
     "shell.execute_reply": "2021-12-20T07:41:20.548069Z",
     "shell.execute_reply.started": "2021-12-20T07:41:20.544387Z"
    },
    "id": "zgs_q_VROkEO"
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EtPXkA5OkEO"
   },
   "source": [
    "* While creating the training step for your model, you will apply Teacher forcing.\n",
    "* Teacher forcing is a technique where the target/real word is passed as the next input to the decoder instead of previous prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:22.620322Z",
     "iopub.status.busy": "2021-12-20T07:41:22.620049Z",
     "iopub.status.idle": "2021-12-20T07:41:22.628596Z",
     "shell.execute_reply": "2021-12-20T07:41:22.627921Z",
     "shell.execute_reply.started": "2021-12-20T07:41:22.620271Z"
    },
    "id": "fHYD-HlQOkEO"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    #hidden = decoder.reset_state(batch_size=target.shape[0]) #we dont have reset_state method\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape: #Record operations for automatic differentiation for implementing backpropagation\n",
    "        #write your code here to do the training steps\n",
    "        encoder_output = encoder(img_tensor)\n",
    "\n",
    "        # Using the teacher forcing technique where the target word is passed as the next input to the decoder\n",
    "        for t in range(1, target.shape[1]):\n",
    "          # passing encoder_output to the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, encoder_output, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "          dec_input = tf.expand_dims(target[:, t], 1)\n",
    "    \n",
    "    avg_loss = (loss / int(target.shape[1])) #we are calculating average loss for every batch\n",
    "\n",
    "    tot_trainables_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    grads = tape.gradient(loss, tot_trainables_variables) # to calculate gradients with respect to every trainable variable\n",
    "\n",
    "    #compute gradients and apply it to the optimizer and backpropagate.\n",
    "    optimizer.apply_gradients(zip(grads, tot_trainables_variables)) \n",
    "        \n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvmyKL2jOkEO"
   },
   "source": [
    "* While creating the test step for your model, you will pass your previous prediciton as the next input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:24.185580Z",
     "iopub.status.busy": "2021-12-20T07:41:24.185012Z",
     "iopub.status.idle": "2021-12-20T07:41:24.193532Z",
     "shell.execute_reply": "2021-12-20T07:41:24.192763Z",
     "shell.execute_reply.started": "2021-12-20T07:41:24.185541Z"
    },
    "id": "oU-xbIUWOkEO"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    #hidden = decoder.reset_state(batch_size=target.shape[0]) #we dont have reset_state method\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape: #Record operations for automatic differentiation.\n",
    "        #write your code here to do the training steps\n",
    "        encoder_output = encoder(img_tensor)\n",
    "\n",
    "        # Using the teacher forcing technique where the target word is passed as the next input to the decoder.\n",
    "        for t in range(1, target.shape[1]):\n",
    "          # passing encoder_output to the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, encoder_output, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, t], predictions) \n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, t], 1)\n",
    "    \n",
    "    avg_loss = (loss / int(target.shape[1]))#we are calculating average loss for every batch\n",
    "\n",
    "    tot_trainables_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    grads = tape.gradient(loss, tot_trainables_variables) # to calculate gradients with respect to every trainable variable\n",
    "\n",
    "    #compute gradients and apply it to the optimizer and backpropagate.\n",
    "    optimizer.apply_gradients(zip(grads, tot_trainables_variables))\n",
    "        \n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:24.926489Z",
     "iopub.status.busy": "2021-12-20T07:41:24.926204Z",
     "iopub.status.idle": "2021-12-20T07:41:24.931467Z",
     "shell.execute_reply": "2021-12-20T07:41:24.930736Z",
     "shell.execute_reply.started": "2021-12-20T07:41:24.926459Z"
    },
    "id": "oY4R_a1kOkEP"
   },
   "outputs": [],
   "source": [
    "def test_loss_cal(test_dataset):\n",
    "    total_loss = 0\n",
    "\n",
    "    #write your code to get the average loss result on your test data\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n",
    "        batch_loss, t_loss = test_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_test_loss=total_loss / test_num_steps\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:41:25.655671Z",
     "iopub.status.busy": "2021-12-20T07:41:25.654891Z",
     "iopub.status.idle": "2021-12-20T07:54:24.204327Z",
     "shell.execute_reply": "2021-12-20T07:54:24.203589Z",
     "shell.execute_reply.started": "2021-12-20T07:41:25.655617Z"
    },
    "id": "RZQUzFq-OkEP"
   },
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "test_loss_plot = []\n",
    "EPOCHS = 15\n",
    "\n",
    "best_test_loss=100\n",
    "for epoch in tqdm(range(0, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_train_loss=total_loss / train_num_steps\n",
    "        \n",
    "    loss_plot.append(avg_train_loss)    \n",
    "    test_loss = test_loss_cal(test_dataset)\n",
    "    test_loss_plot.append(test_loss)\n",
    "    \n",
    "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
    "        best_test_loss = test_loss\n",
    "        ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:54:24.206435Z",
     "iopub.status.busy": "2021-12-20T07:54:24.206008Z",
     "iopub.status.idle": "2021-12-20T07:54:24.393830Z",
     "shell.execute_reply": "2021-12-20T07:54:24.393148Z",
     "shell.execute_reply.started": "2021-12-20T07:54:24.206395Z"
    },
    "id": "zGFKXbJZOkEP"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.plot(test_loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQROpt7wOkEP"
   },
   "source": [
    "#### NOTE: \n",
    "* Since there is a difference between the train & test steps ( Presence of teacher forcing), you may observe that the train loss is decreasing while your test loss is not. \n",
    "* This doesn't mean that the model is overfitting, as we can't compare the train & test results here, as both approach is different.\n",
    "* Also, if you want to achieve better results you can run it more epochs, but the intent of this capstone is to give you an idea on how to integrate attention mechanism with E-D architecture for images. The intent is not to create the state of art model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fhFT5QCOkEP"
   },
   "source": [
    "## Model Evaluation\n",
    "1.Define your evaluation function using greedy search\n",
    "\n",
    "2.Define your evaluation function using beam search ( optional)\n",
    "\n",
    "3.Test it on a sample data using BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pk0054VOkEQ"
   },
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:55:22.900133Z",
     "iopub.status.busy": "2021-12-20T07:55:22.899861Z",
     "iopub.status.idle": "2021-12-20T07:55:22.909861Z",
     "shell.execute_reply": "2021-12-20T07:55:22.909198Z",
     "shell.execute_reply.started": "2021-12-20T07:55:22.900099Z"
    },
    "id": "0GJ8oh20OkEQ"
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    #hidden = decoder.reset_state(batch_size=1)\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0) #process the input image to desired format before extracting features\n",
    "    img_tensor_val = image_features_extract_model(temp_input) # Extract features using our feature extraction model\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)# extract the features by passing the input to encoder\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)# get the output from decoder\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id =  tf.argmax(predictions[0]).numpy()#extract the predicted id(embedded value) which carries the max value\n",
    "        result.append(tokenizer.index_word[predicted_id])#map the id to the word from tokenizer and append the value to the result list\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot,predictions\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot,predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYE4eUpUOkEQ"
   },
   "source": [
    "### Beam Search(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:55:25.312369Z",
     "iopub.status.busy": "2021-12-20T07:55:25.311804Z",
     "iopub.status.idle": "2021-12-20T07:55:25.320110Z",
     "shell.execute_reply": "2021-12-20T07:55:25.319117Z",
     "shell.execute_reply.started": "2021-12-20T07:55:25.312325Z"
    },
    "id": "oa7z2VsNOkEQ"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def plot_attmap(caption, weights, image):\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    temp_img = np.array(Image.open(image))\n",
    "    \n",
    "    len_cap = len(caption)\n",
    "    for cap in range(len_cap):\n",
    "        weights_img = np.reshape(weights[cap], (8,8))\n",
    "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
    "        \n",
    "        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n",
    "        ax.set_title(caption[cap], fontsize=15)\n",
    "        \n",
    "        img=ax.imshow(temp_img)\n",
    "        \n",
    "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
    "        ax.axis('off')\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:55:26.320053Z",
     "iopub.status.busy": "2021-12-20T07:55:26.319772Z",
     "iopub.status.idle": "2021-12-20T07:55:27.694794Z",
     "shell.execute_reply": "2021-12-20T07:55:27.694099Z",
     "shell.execute_reply.started": "2021-12-20T07:55:26.320022Z"
    },
    "id": "RfPxv1cSOkER"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T07:55:27.897614Z",
     "iopub.status.busy": "2021-12-20T07:55:27.897070Z",
     "iopub.status.idle": "2021-12-20T07:55:27.902621Z",
     "shell.execute_reply": "2021-12-20T07:55:27.901846Z",
     "shell.execute_reply.started": "2021-12-20T07:55:27.897575Z"
    },
    "id": "C0rdncPXOkER"
   },
   "outputs": [],
   "source": [
    "def filt_text(text):\n",
    "    filt=['<start>','<unk>','<end>'] \n",
    "    temp= text.split()\n",
    "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
    "    text=' '.join(temp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-20T08:03:47.334469Z",
     "iopub.status.busy": "2021-12-20T08:03:47.334119Z",
     "iopub.status.idle": "2021-12-20T08:03:49.140355Z",
     "shell.execute_reply": "2021-12-20T08:03:49.139554Z",
     "shell.execute_reply.started": "2021-12-20T08:03:47.334418Z"
    },
    "id": "W0u4qya-OkER"
   },
   "outputs": [],
   "source": [
    "# Greedy Search Evaluation on a test image , caption\n",
    "rid = np.random.randint(0, len(path_test))\n",
    "test_image = path_test[rid]\n",
    "test_image, cap_test\n",
    "\n",
    "test_image = path_test[rid]\n",
    "\n",
    "real_caption = cap_test[rid]\n",
    "\n",
    "\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "\n",
    "\n",
    "real_caption=filt_text(real_caption)      \n",
    "\n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(0,0,1,0))\n",
    "print(f\"BLEU score: {score*100}\")\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', pred_caption)\n",
    "plot_attmap(result, attention_plot, test_image)\n",
    "\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ****Summary****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final submission file for the Capstone project - Eye for Blind.\n",
    "An CNN-RNN based Attention model has been built on flickr8k dataset to predict captions for random images. The Model selects captions using Greedy Search and resulting captions are evaluated using BLUE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The project began with reading images and captions.   \n",
    "* Displayed images and captions, stored in new dataframe. \n",
    "* EDA is performed to comprehend about the given dataset.  \n",
    "* Data cleaning is done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is additionally performed which included :\n",
    "\n",
    "* tokenizing the captions and getting an embeded vector,\n",
    "* preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* data is splitted into train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To Extract Features from Images:\n",
    "* InceptionV3 model is used.Inception v3 is a widely-used image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. So let's also use same model to get feature vector.\n",
    "* we are not classifying the images, we only need to extract a feature vector from images.Hence we are removing the softmax layer from the model. \n",
    "* This feature vector is given as input to CNN Encoder \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the output from the encoder, hidden state and start token is passed as input to the decoder. \n",
    "* The decoder goes over the image to predict the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention model:\n",
    "* We utilized the attention model to make our decoder center around a specific piece of the picture at a time rather than focusing in on the whole picture.\n",
    "* This likewise diminishes noise and further improves accuracy. The decoder returns the predicted caption and the decoder's hidden state as output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Loss:\n",
    "* The predictions are utilized to calculate the  loss utilizing cross-entropy \"SparseCategoricalCrossentropy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< end > token:\n",
    "    \n",
    "* The decoder stop predicting when the model get the end token in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the Model works on the prediction of captions for the image:\n",
    "\n",
    "* the prediction of captions by the model is done by finding out the probabbilites of the word in the vocabulary / language model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Greedy search Method:\n",
    "* It calculates the probability of the words according to their occurrence in the vocabulary.\n",
    "* It takes the example of the words, tracks down the probabilities of each word and afterward yields the word with the most noteworthy probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At long last, we are using the \"BLEU score\"(Bilingual Evaluation Understudy) as the evaluation metric for the predicted word. It decides the distinction between the predicted caption and the real caption "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Models works really well. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
